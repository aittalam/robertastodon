{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1504ef-3abc-4e66-88e4-5e838b9f740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Mastodon.py bs4 scikit-learn plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949dcebb-5ba8-40fd-86b6-3e2915c266d9",
   "metadata": {},
   "source": [
    "# 1. Register the Mastodon app\n",
    "\n",
    "The following cell has to be ran only once to register the Mastodon app (you can keep it commented afterwards). The parameters passed to the `create_app` function are:\n",
    "\n",
    "- the name of your app (\"mastimeline\" in this case)\n",
    "- `api_base_url` (the URL of the mastodon server where you have your account)\n",
    "- the file where you want to store the app credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a1271-b3dd-4c6c-98c8-76dbb44d3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mastodon import Mastodon\n",
    "\n",
    "\n",
    "# Mastodon.create_app(\n",
    "#      'mastimeline',\n",
    "#      api_base_url = 'https://fosstodon.org',\n",
    "#      to_file = 'mastimeline_clientcred.secret'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc1799-3fc0-49e3-9b15-23e7a452c0cb",
   "metadata": {},
   "source": [
    "# 2. Log in\n",
    "\n",
    "The code in the following cell is used to log into your Mastodon server:\n",
    "\n",
    "- `client_id` is the name of the file containing the credentials that were generated on the previous step\n",
    "- login and password are loaded from the file `mastimeline_auth.secret`, a simple plaintext file holding login (email) in the first line and password in the second one\n",
    "- finally, the client logs in and stores the access token into the file `mastimeline_usercred.secret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54566ab7-ae6a-44e8-a7e4-f08f37ed859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mastodon import Mastodon\n",
    "\n",
    "mastodon = Mastodon(\n",
    "    client_id = 'mastimeline_clientcred.secret',\n",
    ")\n",
    "\n",
    "with open(\"mastimeline_auth.secret\", \"rt\") as f:\n",
    "    (login,pw) = f.read().split(\"\\n\")\n",
    "\n",
    "mastodon.log_in(\n",
    "    login,\n",
    "    pw,\n",
    "    to_file = 'mastimeline_usercred.secret'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40824f16-d581-4193-8a0d-5afa51b42703",
   "metadata": {},
   "source": [
    "# 3. Create an actual API instance\n",
    "\n",
    "The following code instantiates a Mastodon API client using the user credentials generated in the previous step. The commented line posts a toot from your account, feel free to uncomment it for testing purposes :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273125fa-4c93-4aff-9e2e-d48a3170f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mastodon = Mastodon(\n",
    "    access_token = 'mastimeline_usercred.secret',\n",
    ")\n",
    "# mastodon.toot('Tooting from Python using #mastodonpy, mwahahahah! >:-)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c772cf8-b8d7-4251-96bc-dde9bc172b0d",
   "metadata": {},
   "source": [
    "# 4. Get the revchron timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcf5c9-193e-47da-bb3d-d1d1645404ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = mastodon.timeline_home()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886539cc-26a6-4a82-ba9a-3caca10c538c",
   "metadata": {},
   "source": [
    "# 4.1 Print toots' contents\n",
    "\n",
    "As toots' content is HTML, the following code uses BeautifulSoup to decode it and print it as plain text. Note that in the case of boosts the `toot.content` field will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbf066-8a19-4906-875f-ea535ac8a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# print the content of some toots in the timeline\n",
    "for toot in tl:\n",
    "    soup = BeautifulSoup(toot.content)\n",
    "    text = soup.get_text()\n",
    "    # note some toots might look empty as the \"content\" field does not work for boosts\n",
    "    if text != \"\":\n",
    "        print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85809cda-f281-45bb-918b-b3e4370730de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the content - but better :-)\n",
    "# (the content of boosts appears in the `toot.reblog.content` field so we get it from there)\n",
    "\n",
    "for toot in tl:\n",
    "    id = toot.id\n",
    "    cont = toot.content\n",
    "    if toot.reblog:\n",
    "        id = toot.reblog.id\n",
    "        cont = toot.reblog.content\n",
    "    soup = BeautifulSoup(cont)\n",
    "    print(f\"{id}: {soup.get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b765c5-8afb-4e05-8e58-22384e5d8672",
   "metadata": {},
   "source": [
    "# 5. Understand pagination\n",
    "\n",
    "The `tl` object has two `_pagination_prev` and `_pagination_next` attributes which provide pagination information:\n",
    "- `_pagination_prev` has a `min_id` value which provides the smallest toot id in the current data chunk\n",
    "- `_pagination_next` has a `max_id` value which provides the largest toot id in the current data chunk\n",
    "\n",
    "Starting from the current tl content (which contains the most recent toots), we can paginate back in time by taking the current `min_id` and asking for the data chunk that ends right before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5db8c0-f675-41cd-be02-2158cd8c9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl._pagination_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1d960-5d2b-4e77-84b4-ac4fe0b429bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl._pagination_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bbdd7-eacf-4296-9007-21cd2d543bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = mastodon.timeline_home()\n",
    "data = tl\n",
    "i = 0\n",
    "while len(tl)>0:\n",
    "    i+=1\n",
    "    print(f\"{i}: {tl._pagination_next.get('max_id')}\")\n",
    "    tl = mastodon.timeline_home(max_id = tl._pagination_next.get('max_id'))\n",
    "    data.extend(tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f6e6b-fd09-4b54-88e1-148ed49939de",
   "metadata": {},
   "source": [
    "# 6. Analyze downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dac1fb-828b-4902-8e05-6940f4cb77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "toot_ids = [data[i]['id'] for i in range(len(data))]\n",
    "print(f\"I downloaded {len(toot_ids)} toots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c6472-5108-436d-824f-8165209b49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "text = []\n",
    "\n",
    "for toot in data:\n",
    "    id = toot.id\n",
    "    cont = toot.content\n",
    "    if toot.reblog:\n",
    "        id = toot.reblog.id\n",
    "        cont = toot.reblog.content\n",
    "    soup = BeautifulSoup(cont)\n",
    "    # print(f\"{id}: {soup.get_text()}\")\n",
    "    ids.append(id)\n",
    "    text.append(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ea631-e523-4347-b40f-6ea5ebe376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba70cb4a-4361-48ff-932c-e3f5951d4a00",
   "metadata": {},
   "source": [
    "## 6.1 Calculate text embeddigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8019e5-7b30-482c-af83-6a40d4a89975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "    \n",
    "MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, output_hidden_states=True)\n",
    "class_mapping = model.config.id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e453dcc-4201-49b9-a345-269bf8ea3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "descr = []\n",
    "expits = []\n",
    "lbls = []\n",
    "\n",
    "tt = time.time()\n",
    "for t in text:\n",
    "    i += 1\n",
    "    if not (i%10): \n",
    "        print(\".\", end=\"\")\n",
    "    tokens = tokenizer(t, return_tensors='pt')\n",
    "    output = model(**tokens)\n",
    "\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    descr.append(scores)\n",
    "    scores = expit(scores)\n",
    "    expits.append(scores)\n",
    "\n",
    "    lbls.append (np.argmax(scores))\n",
    "\n",
    "\n",
    "descr = np.array(descr)\n",
    "lbls = np.array(lbls)\n",
    "expits = np.array(expits)\n",
    "\n",
    "# this is the time it takes to calculate 800 embeddings on my \n",
    "# 4-cores 2016 Macbook Pro... 3' is not great, but maybe will\n",
    "# be much smaller on recent hardware (also note that if this\n",
    "# works nicely we can look for other lighter models too!)\n",
    "print(time.time()-tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675fbac-a5fc-4ffb-8e87-1c31db96661f",
   "metadata": {},
   "source": [
    "## 6.2 Use KDTree to calculate K-nearest neighbors of a given status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396aea9-43ba-4e66-9668-53b76cfd8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "tree = spatial.KDTree(descr)\n",
    "\n",
    "# get the 5 nearest neighbors of descr[42]\n",
    "idx = tree.query(descr[10], k=5)[1]\n",
    "for i in idx:\n",
    "    print(f\"{ids[i]}:{text[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b003b-e558-4566-8e65-c5afe826d4a4",
   "metadata": {},
   "source": [
    "## 6.3 Use TSNE to plot the statuses in a 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f79301-1471-47d0-a41b-da08d5819614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "# note you can play with the perplexity parameter to have more or less crisp clusters\n",
    "# (smaller values of perplexity tends to have tighter, more sparse clusters, while \n",
    "# larger values return larger, more globular and possibly overlapping ones)\n",
    "tsne = TSNE(n_components=2, \n",
    "            random_state=42,\n",
    "            perplexity=30\n",
    ")\n",
    "projections = tsne.fit_transform(descr)\n",
    "\n",
    "fig = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    # when hovering, the matching class is shown\n",
    "    hover_name = [class_mapping[lbl] for lbl in lbls],\n",
    "    color=lbls,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9e4cf-4dab-4937-8c0f-194b84bbcb56",
   "metadata": {},
   "source": [
    "Note that what we plotted above are the descriptors /before/ they are mapped with the `scipy.expit` function.\n",
    "For this reason, we also try to directly plot `expits` and see if the plot is more/less meaningful to us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd164a8a-8642-43ae-8080-dc0eb6d74022",
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = tsne.fit_transform(np.array(expits))\n",
    "\n",
    "fig = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    hover_name = [class_mapping[lbl] for lbl in lbls],\n",
    "    color=lbls,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21ec6b-c7ce-4d2d-839f-93bae25d367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### look at the differnce between plain descriptor and expits\n",
    "print(descr[10])\n",
    "print(expits[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375dba4b-2097-4b16-a8e9-4d191403321a",
   "metadata": {},
   "source": [
    "# Tests with mean embeddings\n",
    "\n",
    "We now try to build an embedding for the whole sentence which is the mean of\n",
    "the embeddings calculated for each token.\n",
    "Note that other pre-trained sentence-based approaches (e.g. SBERT https://www.sbert.net/) are available and probably way better than this :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbbc6e-0516-44d6-b867-317f5d1d5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Hello world\"\n",
    "tokens = tokenizer(t, return_tensors='pt')\n",
    "\n",
    "### NOTE: we can skip this in favor of enabling hidden states and taking\n",
    "### the last set of embeddings (hidden_states[12])\n",
    "# bebe = model.get_submodule(\"roberta\")\n",
    "# encoder_outputs = bebe(**tokens)\n",
    "# sequence_output = encoder_outputs[0]\n",
    "# sequence_output\n",
    "\n",
    "output = model(**tokens)\n",
    "# output.hidden_states[-1][:,0].shape\n",
    "mean_embedding = output.hidden_states[-1].mean(axis=1)[0].detach().numpy()\n",
    "mean_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7451b-b669-4fe8-833c-7372d6689e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "tt = time.time()\n",
    "for t in text:\n",
    "    i += 1\n",
    "    if not (i%10): \n",
    "        print(\".\", end=\"\")\n",
    "    tokens = tokenizer(t, return_tensors='pt')\n",
    "    output = model(**tokens)\n",
    "    mean_embedding = output.hidden_states[-1].mean(axis=1)[0].detach().numpy()\n",
    "    embeddings.append(mean_embedding)\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(time.time()-tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d65d60-ae32-43c2-91ce-4d301787f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "tree2 = spatial.KDTree(embeddings)\n",
    "\n",
    "idx = tree2.query(embeddings[404], k=5)[1]\n",
    "for i in idx:\n",
    "    print(f\"{ids[i]}:{text[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f88ff-bf5f-4b5c-9313-c654e835f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, \n",
    "            random_state=42,\n",
    "            perplexity=30\n",
    ")\n",
    "projections = tsne.fit_transform(embeddings)\n",
    "\n",
    "fig = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    # here the status index is prepended to the class when hovering, \n",
    "    # allowing one to check out its neighbors using the code in the \n",
    "    # previous cell (or the next one, if you want to compare mean \n",
    "    # \"embeddings\" with default \"descr\")\n",
    "    hover_name = [f\"{idx} - {class_mapping[lbl]}\" for idx, lbl in enumerate(lbls)],\n",
    "    color=lbls,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d41de-1075-4efc-8f95-b0aab7903d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "tree = spatial.KDTree(descr)\n",
    "\n",
    "idx = tree.query(descr[404], k=5)[1]\n",
    "for i in idx:\n",
    "    print(f\"{ids[i]}:{text[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b747eb2-1d55-49a5-a34e-09dd7d75558b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
